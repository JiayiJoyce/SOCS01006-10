pacman::p_load(tidyverse, # tidyverse pkgs including purrr
purrr, # automating
xml2, # parsing XML
rvest, # parsing HTML
robotstxt)
install.packages("robotstxt")
install.packages("robotstxt")
install.packages("rvest")
install.packages("xml2")
url <- "https://en.wikipedia.org/wiki/University_College_London"
parsed <- read_html(url)
pacman::p_load(tidyverse, # tidyverse pkgs including purrr
purrr, # automating
xml2, # parsing XML
rvest, # parsing HTML
robotstxt)
url <- "https://en.wikipedia.org/wiki/University_College_London"
parsed <- read_html(url)
View(parsed)
url <- "https://en.wikipedia.org/wiki/University_College_London"
parsed <- read_html(url)
parsed.sub <- html_element(parsed, xpath = '//*[@id="mw-content-text"]/div[1]/table[1]')
table.df <- html_table(parsed.sub)
head(table.df)
pacman::p_load(tidyverse, # tidyverse pkgs including purrr
purrr, # automating
xml2, # parsing XML
rvest, # parsing HTML
robotstxt,
janitor)
# clean names
names(table.df) <-  janitor::make_clean_names(names(table.df))
# Delete empty rows
empt <- apply(table.df, 1, FUN = function(x) all(is.na(x) | x == ""))
table.df <- table.df[which(!empt), ]
# Exclude empty columns
table.df <- table.df[,-3:-7]
head(table.df)
paths_allowed(paths = "https://en.wikipedia.org/wiki/University_of_Cambridge")
paths_allowed(paths = "https://en.wikipedia.org/wiki/University_of_Cambridge")
#creating url list for the websites to be scraped
url_list <- c(
"https://en.wikipedia.org/wiki/University_College_London",
"https://en.wikipedia.org/wiki/University_of_Cambridge",
"https://en.wikipedia.org/wiki/University_of_Oxford"
)
url <- "https://en.wikipedia.org/wiki/University_College_London"
download.file(url, destfile = "scraped_page.html", quiet = TRUE)
target <- read_html("scraped_page.html")
# If you want character vector output
target1 <- target %>%
html_nodes(xpath = '//*[@id="mw-content-text"]/div[1]/table[1]') %>%
html_text()
# If you want character vector output
target1 <- target %>%
html_nodes(xpath = '//*[@id="mw-content-text"]/div[1]/table[1]') %>%
html_text()
#Automation
library(purrr)
map(url_list, get_table_from_wiki)
# If you want table output
target2 <- target %>%
html_nodes(xpath = '//*[@id="mw-content-text"]/div[1]/table[1]') %>%
html_table()
get_table_from_wiki <- function(url){
download.file(url, destfile = "scraped_page.html", quiet = TRUE)
target <- read_html("scraped_page.html")
table <- target %>%
html_nodes(xpath = '//*[@id="mw-content-text"]/div[1]/table[1]') %>%
html_table()
return(table)
}
# Automating data collection-----
#Testing
get_table_from_wiki(url_list[[2]])
#Automation
library(purrr)
map(url_list, get_table_from_wiki)
pacman::p_load(tidyverse, # tidyverse pkgs including purrr
purrr, # automating
xml2, # parsing XML
rvest, # parsing HTML
robotstxt,
janitor)
paths_allowed(paths="https://finance.yahoo.com/world-indices/?guccounter=1")
yahoo_finance_url <- "https://finance.yahoo.com/world-indices/?guccounter=1"
parsed <- read_html(yahoo_finance_url)
yahoo_finance_url <- "https://finance.yahoo.com/world-indices/?guccounter=1"
parsed <- read_html(yahoo_finance_url)
parsed.sub <- html_element(parsed, xpath = '//*[@id="yfin-list"]/div[2]')
table.df <- html_table(parsed.sub)
head(table.df)
table.df<- janitor::make_clean_names(names(table.df))
head(table.df)
table.df<- janitor::make_clean_names(names(table.df))
table.df<- janitor::make_clean_names(names(table.df))%>%
select(table.df,name,last_price,change)
table.df<- janitor::make_clean_names(names(table.df))%>%
select(name,last_price,change)
NLC<-table.df%>%
select(name,last_price,change)
NLC<-table.df%>%
select("name","last_price","change")
pacman::p_load(tidyverse, # tidyverse pkgs including purrr
purrr, # automating
xml2, # parsing XML
rvest, # parsing HTML
robotstxt,
janitor,
dplyr)
table.df<- janitor::make_clean_names(names(table.df))
NLC<-table.df%>%
select("name","last_price","change")
pacman::p_load(tidyverse, # tidyverse pkgs including purrr
purrr, # automating
xml2, # parsing XML
rvest, # parsing HTML
robotstxt,
janitor,
dplyr)
table.df<- janitor::make_clean_names(names(table.df))
NLC<-table.df%>%
select("name","last_price","change")
NLC<-table.df%>%
select("name","last_price","change")
table.df <- html_table(parsed.sub)
head(table.df)
table.df<- janitor::make_clean_names(names(table.df))
head(table.df)
table.df<- janitor::make_clean_names(names(table.df))
head(table.df)
NLC<-table.df%>%
select("name","last_price","change")
table.df<- janitor::make_clean_names(names(table.df))
head(table.df)
NLC<-table.df%>%
select(name,last_price,change)
NLC<-table.df%>%
select(name,last_price,change)
NLC<-table.df%>%
select(name,last_price,change)
parsed.sub <- html_element(parsed, xpath = '//*[@id="list-res-table"]/div[1]/table')
table.df <- html_table(parsed.sub)
head(table.df)
table.df<- janitor::make_clean_names(names(table.df))
head(table.df)
NLC<-table.df%>%
select(name,last_price,change)
View(parsed)
View(parsed.sub)
table.df <- data.frame(table.df)
table.df<- janitor::make_clean_names(names(table.df))
head(table.df)
NLC<-table.df%>%
select(name,last_price,change)
table.df <- data.frame(table.df)
View(table.df)
yahoo_finance_url <- "https://finance.yahoo.com/world-indices/?guccounter=1"
parsed <- read_html(yahoo_finance_url)
parsed.sub <- html_element(parsed, xpath = '//*[@id="list-res-table"]/div[1]/table')
table.df <- html_table(parsed.sub)
View(table.df)
NLC<-table.df%>%
select(Name,`Last Price`,`% Change`)
NLC<-table.df%>%
select(Name,`Last Price`,`% Change`)
name(NLC)<- janitor::make_clean_names(names(NLC))
name(NLC) <- janitor::make_clean_names(names(NLC))
names(NLC) <- janitor::make_clean_names(names(NLC))
head(NLC)
NLC$percent_change <-as.numeric(gsub("%","",NLC$percent_change))
pacman::p_load(tidyverse, # tidyverse pkgs including purrr
purrr, # automating
xml2, # parsing XML
rvest, # parsing HTML
robotstxt,
janitor,
dplyr,
plotly)
NLC$percent_change <-as.numeric(gsub("%","",NLC$percent_change))
plot_ly(NLC,X= ~name, y=~price, type='bar') %>%
layout(title="Stock indices prices and changes",
xaxis= list(title="Name"),
yaxis= list(title="price"))
NLC$percent_change <-as.numeric(gsub("%","",NLC$percent_change))
plot_ly(NLC,X= ~name, y=~last_price, type='bar') %>%
layout(title="Stock indices prices and changes",
xaxis= list(title="Name"),
yaxis= list(title="price"))
NLC$percent_change <-as.numeric(gsub("%","",NLC$percent_change))
plot_ly(NLC,X= ~name, y=~last_price, type='bar',color = percent_change) %>%
layout(title="Stock indices prices and changes",
xaxis= list(title="Name"),
yaxis= list(title="price"))
NLC<-table.df%>%
select(Name,`Last Price`,`% Change`)
#name,last price change
names(NLC) <- janitor::make_clean_names(names(NLC))
head(NLC)
NLC$percent_change <-as.numeric(gsub("%","",NLC$percent_change))
NLC$percent_change <-as.numeric(gsub("%","",NLC$percent_change))
plot_ly(NLC,X= ~name, y=~last_price, type='bar',color = ~percent_change) %>%
layout(title="Stock indices prices and changes",
xaxis= list(title="Name"),
yaxis= list(title="price"))
NLC<-NLC(order(last_price))
NLC<-NLC[order(last_price)]
NLC<-NLC[order(NLC$last_price)]
NLC<-NLC[order(NLC$last_price),]
NLC$percent_change <-as.numeric(gsub("%","",NLC$percent_change))
plot_ly(NLC,X= ~name, y=~last_price, type='bar',color = ~percent_change) %>%
layout(title="Stock indices prices and changes",
xaxis= list(title="Name"),
yaxis= list(title="price"))
NLC$percent_change <-as.numeric(gsub("%","",NLC$percent_change))
plot_ly(NLC,X= ~name, y=~last_price, type='bar',color = ~percent_change) %>%
layout(title="Stock indices prices and changes",
xaxis= list(title="Name"),
yaxis= list(title="price"))
NLC$percent_change <-as.numeric(gsub("%","",NLC$percent_change))
plot_ly(NLC,X="~name", y=~last_price, type='bar',color = ~percent_change) %>%
layout(title="Stock indices prices and changes",
xaxis= list(title="Name"),
yaxis= list(title="price"))
View(NLC)
NLC<-NLC[order(NLC$last_price),]
NLC$percent_change <-as.numeric(gsub("%","",NLC$percent_change))
plot_ly(NLC,X=~name, y=~last_price, type='bar',color = ~percent_change) %>%
layout(title="Stock indices prices and changes",
# xaxis= list(title="Name"),
yaxis= list(title="price"))
paths_allowed(paths="https://finance.yahoo.com/world-indices/?guccounter=1")
yahoo_finance_url <- "https://finance.yahoo.com/world-indices/?guccounter=1"
parsed <- read_html(yahoo_finance_url)
parsed.sub <- html_element(parsed, xpath = '//*[@id="list-res-table"]/div[1]/table')
table.df <- html_table(parsed.sub)
head(table.df)
NLC<-table.df%>%
select(Name,`Last Price`,`% Change`)
#name,last price change
names(NLC) <- janitor::make_clean_names(names(NLC))
head(NLC)
#can scrape the table directly through html_nodes
# alternative to scraping entire- can scrape individual function- retrieve each coloum as a list
#then combine each one as a list
# NLC<-NLC[order(NLC$last_price),]
NLC$percent_change <-as.numeric(gsub("%","",NLC$percent_change))
plot_ly(NLC,X=~name, y=~last_price, type='bar',color = ~percent_change) %>%
layout(title="Stock indices prices and changes",
# xaxis= list(title="Name"),
yaxis= list(title="price"))
NLC$percent_change <-as.numeric(gsub("%","",NLC$percent_change))
plot_ly(NLC,x=~name, y=~last_price, type='bar',color = ~percent_change) %>%
layout(title="Stock indices prices and changes",
xaxis= list(title="Name"),
yaxis= list(title="price"))
rstudioapi::addTheme("https://raw.githubusercontent.com/jnolis/synthwave85/master/Synthwave85.rstheme", TRUE, TRUE, FALSE)
shiny::runApp('myfirstshinyapp')
devtools::install_github("thomasp85/scico")
pacman::p_load(tidyverse,
xml2,
rvest,
robotstxt,
shiny,
janitor,
scico)
devtools::install_github("calligross/ggthemeassist")
ggThemeAssist:::ggThemeAssistAddin()
ggThemeAssist:::ggThemeAssistAddin()
ggThemeAssist:::ggThemeAssistAddin()
ggThemeAssist:::ggThemeAssistAddin()
filtered_df <- df %>% filter(country %in% c("Brazil", "Turkey", "Argentina", "Afghanistan", "Belgium"))
setwd("/Users/chenjiayi/Desktop/Computational/week04")
rm(list = ls())
#setup
if (!require("pacman")) {
install.packages("pacman")
}
pacman::p_load(
tidyverse, # tidyverse pkgs including purrr
kableExtra,#table
flextable, #table
glue, #combining strings and objects
ggplot2) #dataviz
options(scipen=999) #this is to avoid scientific notation in results
# importing data in .csv format
data <- read.csv("/Users/chenjiayi/Desktop/Computational/PublicRepoforSOCS0100Wk3/EMDAT.csv", header = TRUE)
# Subsetting and renaming data before automating the tasks
df <- data %>% select(Entity, Year, deaths_all_disasters, injured_all_disasters, homeless_all_disasters) %>%
rename(deaths = deaths_all_disasters, injuries = injured_all_disasters,
homelessness = homeless_all_disasters, country = Entity)
## Using purrr, please automate at least one data wrangling task based on the dataset
#(e.g. summarising data)
df %>%
select(-country, -Year) %>%  # Remove non numerical columns
map_dbl(mean, na.rm = TRUE)
## Using purrr please automate plotting the trends of deaths, injuries,
##and homelessness caused by all disasters for 5 countries in the dataset
filtered_df <- df %>% filter(country %in% c("Brazil", "Turkey", "Argentina", "Afghanistan", "Belgium"))
create_point_plot <- function(i) {
filtered_df %>%
ggplot(aes_string(x = names(filtered_df)[2], y = names(filtered_df)[i])) +
geom_point() +
geom_smooth(method = "lm", se = TRUE, color = "blue") +  # Add trend lin
labs(
title = glue("The Trend of {names(df)[i]}"),
y = glue("{names(df)[i]}")
)
}
plots_list <- map(3:ncol(filtered_df), create_point_plot)
plots_grid <- gridExtra::grid.arrange(grobs = plots_list, ncol = 2) # Adjust ncol as needed
setwd("/Users/chenjiayi/Desktop/Computational/week04")
rm(list = ls())
#setup
if (!require("pacman")) {
install.packages("pacman")
}
pacman::p_load(
tidyverse, # tidyverse pkgs including purrr
kableExtra,#table
flextable, #table
glue, #combining strings and objects
ggplot2) #dataviz
options(scipen=999) #this is to avoid scientific notation in results
# importing data in .csv format
data <- read.csv("/Users/chenjiayi/Desktop/Computational/PublicRepoforSOCS0100Wk3/EMDAT.csv", header = TRUE)
# Subsetting and renaming data before automating the tasks
df <- data %>% select(Entity, Year, deaths_all_disasters, injured_all_disasters, homeless_all_disasters) %>%
rename(deaths = deaths_all_disasters, injuries = injured_all_disasters,
homelessness = homeless_all_disasters, country = Entity)
## Using purrr, please automate at least one data wrangling task based on the dataset
#(e.g. summarising data)
df %>%
select(-country, -Year) %>%  # Remove non numerical columns
map_dbl(mean, na.rm = TRUE)
## Using purrr please automate plotting the trends of deaths, injuries,
##and homelessness caused by all disasters for 5 countries in the dataset
filtered_df <- df %>% filter(country %in% c("Brazil", "Turkey", "Argentina", "Afghanistan", "Belgium"))
create_point_plot <- function(i) {
filtered_df %>%
ggplot(aes_string(x = names(filtered_df)[2], y = names(filtered_df)[i])) +
geom_point() +
geom_smooth(method = "lm", se = TRUE, color = "blue") +  # Add trend lin
labs(
title = glue("The Trend of {names(df)[i]}"),
y = glue("{names(df)[i]}")
)
}
plots_list <- map(3:ncol(filtered_df), create_point_plot)
plots_grid <- gridExtra::grid.arrange(grobs = plots_list, ncol = 2) # Adjust ncol as needed
setwd("/Users/chenjiayi/Desktop/Computational/week04")
rm(list = ls())
#setup
if (!require("pacman")) {
install.packages("pacman")
}
pacman::p_load(
tidyverse, # tidyverse pkgs including purrr
kableExtra,#table
flextable, #table
glue, #combining strings and objects
ggplot2) #dataviz
options(scipen=999) #this is to avoid scientific notation in results
# importing data in .csv format
data <- read.csv("/Users/chenjiayi/Desktop/Computational/PublicRepoforSOCS0100Wk3/EMDAT.csv", header = TRUE)
# Subsetting and renaming data before automating the tasks
df <- data %>% select(Entity, Year, deaths_all_disasters, injured_all_disasters, homeless_all_disasters) %>%
rename(deaths = deaths_all_disasters, injuries = injured_all_disasters,
homelessness = homeless_all_disasters, country = Entity)
## Using purrr, please automate at least one data wrangling task based on the dataset
#(e.g. summarising data)
df %>%
select(-country, -Year) %>%  # Remove non numerical columns
map_dbl(mean, na.rm = TRUE)
## Using purrr please automate plotting the trends of deaths, injuries,
##and homelessness caused by all disasters for 5 countries in the dataset
filtered_df <- df %>% filter(country %in% c("Brazil", "Turkey", "Argentina", "Afghanistan", "Belgium"))
create_point_plot <- function(i) {
filtered_df %>%
ggplot(aes_string(x = names(filtered_df)[2], y = names(filtered_df)[i])) +
geom_point() +
geom_smooth(method = "lm", se = TRUE, color = "blue") +  # Add trend lin
labs(
title = glue("The Trend of {names(df)[i]}"),
y = glue("{names(df)[i]}")
)
}
plots_list <- map(3:ncol(filtered_df), create_point_plot)
plots_grid <- gridExtra::grid.arrange(grobs = plots_list, ncol = 2) # Adjust ncol as needed
setwd("/Users/chenjiayi/Desktop/Computational/week04")
rm(list = ls())
#setup
if (!require("pacman")) {
install.packages("pacman")
}
pacman::p_load(
tidyverse, # tidyverse pkgs including purrr
kableExtra,#table
flextable, #table
glue, #combining strings and objects
ggplot2) #dataviz
options(scipen=999) #this is to avoid scientific notation in results
# importing data in .csv format
data <- read.csv("/Users/chenjiayi/Desktop/Computational/PublicRepoforSOCS0100Wk3/EMDAT.csv", header = TRUE)
# Subsetting and renaming data before automating the tasks
df <- data %>% select(Entity, Year, deaths_all_disasters, injured_all_disasters, homeless_all_disasters) %>%
rename(deaths = deaths_all_disasters, injuries = injured_all_disasters,
homelessness = homeless_all_disasters, country = Entity)
## Using purrr, please automate at least one data wrangling task based on the dataset
#(e.g. summarising data)
df %>%
select(-country, -Year) %>%  # Remove non numerical columns
map_dbl(mean, na.rm = TRUE)
## Using purrr please automate plotting the trends of deaths, injuries,
##and homelessness caused by all disasters for 5 countries in the dataset
filtered_df <- df %>% filter(country %in% c("Brazil", "Turkey", "Argentina", "Afghanistan", "Belgium"))
create_point_plot <- function(i) {
filtered_df %>%
ggplot(aes_string(x = names(filtered_df)[2], y = names(filtered_df)[i])) +
geom_point() +
geom_smooth(method = "lm", se = TRUE, color = "blue") +  # Add trend lin
labs(
title = glue("The Trend of {names(df)[i]}"),
y = glue("{names(df)[i]}")
)
}
plots_list <- map(3:ncol(filtered_df), create_point_plot)
plots_grid <- gridExtra::grid.arrange(grobs = plots_list, ncol = 2) # Adjust ncol as needed
ggThemeAssist:::ggThemeAssistAddin()
ggThemeAssist:::ggThemeAssistAddin()
ggThemeAssist:::ggThemeAssistAddin()
#remove everything in your environment and setting up directory
setwd("/Users/chenjiayi/Desktop/Computational/PublicRepoforSOCS0100Wk3")
rm(list = ls())
#setup
if (!require("pacman")) {
install.packages("pacman")
}
pacman::p_load(
tidyverse, # tidyverse pkgs including purrr
kableExtra,#table
flextable, #table
skimr) #a broad overview of data frame
data <- read.csv("EMDAT.csv", header = TRUE)
# 3) Inspect the data briefly and identify its structure
#tip: to avoid all scientific notation (e numbers) by setting options(scipen=999) at the top of the script
skim(data)
df <- data %>% select(Entity, Year, deaths_all_disasters, injured_all_disasters, homeless_all_disasters) %>%
rename(deaths = deaths_all_disasters, injuries = injured_all_disasters,
homelessness = homeless_all_disasters, country = Entity)
glimpse(df)
# Calculate the averages
averages <- df %>%
filter(!country %in% c("World", "Soviet Union")) %>%  # Remove "World" and "Soviet Union"
group_by(country) %>%
summarise(
avg_deaths = mean(deaths, na.rm = TRUE),
avg_injuries = mean(injuries, na.rm = TRUE),
avg_homelessness = mean(homelessness, na.rm = TRUE)
)
# Create tables for the top 10 averages
top_10_deaths <- averages %>%
arrange(desc(avg_deaths)) %>%
head(10) %>%
kable(caption = "Top 10 Countries by Average Deaths")
# Apply formatting
top_10_deaths %>%
kable_styling("striped") %>%
kable_classic(full_width = FALSE)
top_10_injuries <- averages %>%
arrange(desc(avg_injuries)) %>%
head(10) %>%
kable(caption = "Top 10 Countries by Average Injuries")
top_10_injuries %>%
kable_styling("striped") %>%
kable_classic(full_width = FALSE)
top_10_homelessness <- averages %>%
arrange(desc(avg_homelessness)) %>%
head(10) %>%
kable(caption = "Top 10 Countries by Average Homelessness")
top_10_homelessness %>%
kable_styling("striped") %>%
kable_classic(full_width = FALSE)
df <- df %>% mutate(high_death = ifelse(deaths > 500, 1, 0))
# 7) Reshape the dataset (selected version) and save it as a separate dataset in your repository
df_wide <- df %>%
pivot_wider(
names_from = Year,       # Specify the columns to pivot
values_from = c(deaths, injuries, homelessness, high_death)  # Specify the values columns
)
# Save the df_wide data frame as a separate R data set
saveRDS(df_wide, "df_wide.rds")
# 0. Set up: installing packages, remove the # in front of them to run-----
#set working directory
setwd("/Users/chenjiayi/Desktop/Computational/DVHZ3-SOCS0100-Assignment")
#clearing environment
rm(list = ls())
#install Groundhog if it is not installed, remove # to run
if (!require("groundhog")) {
install.packages("groundhog")
}
#creating the vector pkgs that include all the packages needed for the following task
pkgs <- c("tidyverse",
"kableExtra",
"flextable",
"skimr",
"ggplot2",
"glue",
"janitor",
"countrycode")
# install packages as they are available on 2023-11-04, remove # to install
groundhog.library(pkgs, "2023-11-04")
